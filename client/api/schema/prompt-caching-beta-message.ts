/*
 * Copyright (c) 2024 SAP SE or an SAP affiliate company. All rights reserved.
 *
 * This is a generated file powered by the SAP Cloud SDK for JavaScript.
 */
import type { ContentBlock } from './content-block.js';
import type { Model } from './model.js';
import type { PromptCachingBetaUsage } from './prompt-caching-beta-usage.js';
/**
 * Representation of the 'PromptCachingBetaMessage' schema.
 */
export type PromptCachingBetaMessage = {
  /**
   * Unique object identifier.
   *
   * The format and length of IDs may change over time.
   */
  id: string;
  /**
   * Object type.
   *
   * For Messages, this is always `"message"`.
   * Default: "message".
   */
  type: 'message';
  /**
   * Conversational role of the generated message.
   *
   * This will always be `"assistant"`.
   * Default: "assistant".
   */
  role: 'assistant';
  /**
   * Content generated by the model.
   *
   * This is an array of content blocks, each of which has a `type` that determines its shape.
   *
   * Example:
   *
   * ```json
   * [{"type": "text", "text": "Hi, I'm Claude."}]
   * ```
   *
   * If the request input `messages` ended with an `assistant` turn, then the response `content` will continue directly from that last turn. You can use this to constrain the model's output.
   *
   * For example, if the input `messages` were:
   * ```json
   * [
   *   {"role": "user", "content": "What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun"},
   *   {"role": "assistant", "content": "The best answer is ("}
   * ]
   * ```
   *
   * Then the response `content` might be:
   *
   * ```json
   * [{"type": "text", "text": "B)"}]
   * ```
   */
  content: ContentBlock[];
  model: Model;
  /**
   * The reason that we stopped.
   *
   * This may be one the following values:
   * * `"end_turn"`: the model reached a natural stopping point
   * * `"max_tokens"`: we exceeded the requested `max_tokens` or the model's maximum
   * * `"stop_sequence"`: one of your provided custom `stop_sequences` was generated
   * * `"tool_use"`: the model invoked one or more tools
   *
   * In non-streaming mode this value is always non-null. In streaming mode, it is null in the `message_start` event and non-null otherwise.
   */
  stop_reason: 'end_turn' | 'max_tokens' | 'stop_sequence' | 'tool_use' | any;
  /**
   * Which custom stop sequence was generated, if any.
   *
   * This value will be a non-null string if one of your custom stop sequences was generated.
   */
  stop_sequence: string | any;
  /**
   * Billing and rate-limit usage.
   *
   * Anthropic's API bills and rate-limits by token counts, as tokens represent the underlying cost to our systems.
   *
   * Under the hood, the API transforms requests into a format suitable for the model. The model's output then goes through a parsing stage before becoming an API response. As a result, the token counts in `usage` will not match one-to-one with the exact visible content of an API request or response.
   *
   * For example, `output_tokens` will be non-zero, even for an empty string response from Claude.
   */
  usage: PromptCachingBetaUsage;
} & Record<string, any>;
